# Small configuration for quick experimentation and testing

# Model Architecture (smaller for faster training)
model:
  d_signal: 32 # Reduced signal dimension
  m_noise: 16 # Reduced noise dimension
  k_vec: 8 # Reduced vector dimension
  layers: 12 # Fewer layers
  heads: 8 # Fewer attention heads
  rank: 32 # Smaller Lie algebra rank
  sigma: 1.0 # Noise scale

# Training Hyperparameters (faster iteration)
training:
  batch_size: 32 # Smaller batch size
  sequence_length: 256 # Shorter sequences
  learning_rate: 0.0005 # Slightly higher LR for faster convergence
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip_norm: 1.0
  max_epochs: 20 # Fewer epochs for testing
  warmup_steps: 500 # Shorter warmup
  eval_interval: 1000 # More frequent evaluation
  save_interval: 7000 # More frequent saves

# Data Configuration
data:
  dataset_name: "wikitext"
  dataset_config: "wikitext-103-raw-v1"
  tokenizer_name: "gpt2"
  num_workers: 2 # Fewer workers
  pin_memory: true

# Experiment Tracking
experiment:
  project_name: "secure-transformer-small"
  experiment_name: null
  checkpoint_dir: "./checkpoints_small"
  log_level: "INFO"

# Hardware
hardware:
  device: "auto"
